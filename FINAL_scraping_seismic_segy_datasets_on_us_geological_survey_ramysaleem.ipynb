{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Seismic SEGY Datasets on U.S. Geological Survey (USGS)\n",
    "\n",
    "\n",
    "### Sharing Geosciences Data for a changing world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement:\n",
    "Datasets are essential for all scientists to perform research, especially coders geoscientists. There are massive open-source data available online. However, it is often challenging for students and researchers to navigate through the datasets to access them. Because mainly data discoverability is poor, documentation is sometimes lacking, and licences can be unclear. I hope with this project to add toward the solution of these problems, explore all the seismic surveys available seismic data on the USGS website, and contribute by providing 600 seismic surveys in SEGY format ready to download with all the necessary navigation metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Seismic line "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example of Seismic line.png](https://i.imgur.com/gdbNa4V.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of 3D seismic cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example of 3D seismic cube](https://i.imgur.com/0Wl1vJN.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "The page https://walrus.wr.usgs.gov/namss/ provides a map with search filters which contain a list of 600 seismic surveys. In this project, we will retrieve data information and seismic zip files from this page using web scraping: the process of extracting information from a website in an automated fashion using code. We will use the Python libraries `Requests` (https://pypi.org/project/requests/) and `Beautiful Soup` (https://www.crummy.com/software/BeautifulSoup/bs4/doc/)to scrape data from this page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seismic data:\n",
    "Seismic data basically is a large ultrasound of the underground. Geophysical imaging (also known as geophysical tomography) is a minimally destructive geophysical technique that investigates the subsurface of a terrestrial planet. Geophysical imaging is a noninvasive imaging technique with a high parametrical and Spatio-temporal resolution. Geophysical imaging has evolved over the last 30 years due to advances in computing power and speed. It can be used to model a surface or object understudy in 2D or 3D as well as monitor changes.\n",
    "For more info refer to the link: https://en.wikipedia.org/wiki/Geophysical_imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USGS:\n",
    "USGS is the sole science agency for the Department of the Interior. It is sought out by thousands of partners and customers for its natural science expertise and its vast earth and biological data holdings. As the Nation's largest water, earth, and biological science and civilian mapping agency, USGS collects, monitors, analyzes, and provides science about natural resource conditions, issues, and problems. For more inof refer to the link:https://www.usgs.gov/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pacific Coastal and Marine Science Center - The National Archive of Marine Seismic Surveys (NAMSS)\n",
    "The National Archive of Marine Seismic Surveys (NAMSS) is a marine seismic reflection data archive consisting of data acquired by or contributed to U.S. Department of the Interior agencies. The USGS is committed to preserving these data on behalf of the academic community and the nation. Data are provided with free and open access. For more information regarding NAMSS, see the link: https://walrus.wr.usgs.gov/namss/. NAMSS is a massive website for open-source 2D and 3D seismic reflection data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is web scrabing?\n",
    "Web scraping is a term used to describe the use of a program or algorithm to extract and process large amounts of data from the web. Whether you are a data scientist, engineer, or anybody who analyzes large amounts of datasets, the ability to scrape data from the web is a useful skill to have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is python used for scraping?\n",
    "Automated web scraping can be a solution to speed up the data collection process. You write your code once and it will get the information you want many times and from many pages. Python is a popular and best programming language for web scraping. Python can handle multiple data crawling or web scraping tasks comfortably. `Requests` and `BeautifulSoup`, are the most famous and widely used Python frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project workflow:\n",
    "1. Choose a website and describe the project objective\n",
    "2. Create a list with all the seismic surveys URLs using the charactors from a to z.\n",
    "3. Download the webpage using requests.\n",
    "4. Parse the HTML source code using beautiful soup\n",
    "5. Extract surveys names, information and URLs from page\n",
    "6. Compile extracted information into Python lists and dictionaries\n",
    "7. Extract and combine data from multiple survey pages\n",
    "8. Save the extracted information to a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results:\n",
    "By the end of the project, we will create a CSV file in the folowing format:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survey name,Operator,Dates,Data type,Datum,North lat,South lat,East long,West long,SEGY size,Navigation size,SEGY zip,Navigation zip,url\n",
    "B-00-95-LA,Bureau of Ocean Energy Management,1995,3D Multichannel Seismic,North American Datum 1927 (NAD27),28.00779,27.94530,-92.09660,-92.18261,(248.5 MB),(1.4 KB),https://walrus.wr.usgs.gov/namss/data/1995/namss.B-00-95-LA.mcs3d.airgun.zip,https://walrus.wr.usgs.gov/namss/media/navigation/2021/03/16/114121798695/B-00-95-LA.zip,https://walrus.wr.usgs.gov/namss/survey/b-00-95-la\n",
    "B-01-75-AT,Bureau of Ocean Energy Management,1975,2D Multichannel Seismic,North American Datum 1983 (NAD83),28.32871,28.19741,-90.11074,-90.29505,(2.4 GB),(564.6 KB),https://walrus.wr.usgs.gov/namss/data/1975/namss.B-01-75-AT.mcs.airgun.zip,https://walrus.wr.usgs.gov/namss/media/navigation/2015/08/31/b-01-75-at.segp1,https://walrus.wr.usgs.gov/namss/survey/b-01-75-at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example.png](https://i.imgur.com/PCctt2O.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runing the code:\n",
    "You can execute the code using the \"Run\" button at the top of this page. You can make changes and save your own version of the naotebook to [Jovian](https://www.jovian,ai) by executing the folowing cells. Then Run-on Binder, or Colab (Google's cloud infrastructure), or Run-on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Updating notebook \"ramysaleem/scraping-seismic-segy-datasets-on-us-geological-survey-ramysaleem\" on https://jovian.ai/\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Capturing environment..\n",
      "[jovian] Committed successfully! https://jovian.ai/ramysaleem/scraping-seismic-segy-datasets-on-us-geological-survey-ramysaleem\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/ramysaleem/scraping-seismic-segy-datasets-on-us-geological-survey-ramysaleem'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(filename=\"scraping_seismic_segy_datasets_on_us_geological_survey_usgsramysaleem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. USGS website and NAMSS web page detailed  workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first step of the project is to scrape the USGS Science Explorer data website https://www.usgs.gov/science-explorer-results?es=seismic+reflection&classification=data\n",
    "\n",
    "2. Second, we got a list of multiple geosciences websites that contain all the available data where we selected the (NAMSS) https://walrus.wr.usgs.gov/namss/, which contain the seismic reflection data we are interested in. \n",
    "\n",
    "3. Third, we scraped the (NAMSS) website to collect 600 2D and 3D seismic data surveys.\n",
    "\n",
    "4. Later, we got the survey name, name of the operator that shoots the seismic data, dates of acquisition, datum, the coordinates of the surveys as latitude and longitude, size of zipping seismic SEGY files, size of the navigation files, seismic SEGY zip files, navigation zip files and URL for additional information.\n",
    "\n",
    "5. The (NAMSS) website contain a search map icon that opens the https://walrus.wr.usgs.gov/namss/search/ web page. This map has multiple filters which contain and hide all the seismic surveys.\n",
    "\n",
    "6. We have used the programmer inspect tool to identify the base URL and attached \"a\" to \"z\" characters to get all the surveys names. After that, we append them into one list, clean it and add the surveys names to the base URL to get all the surveys web pages.\n",
    "\n",
    "7. From the surveys web pages https://walrus.wr.usgs.gov/namss/survey/b-49-95-la/, we start collecting the needed information.\n",
    "\n",
    "8. We have created a function to get the survey name using the `h1` tag.\n",
    "\n",
    "9. Another function was implemented to get the survey information such as operator, dates, data type, datum using the `div` tag.\n",
    "\n",
    "10. Also, we have created a function that collects the size of the seismic SEGY file and the size of the navigation file using the `span` tag.\n",
    "    \n",
    "11. Moreover, we have scraped the navigation metadata XML web page to collect the coordinates as latitude and longitude using the `northlat`, `southlat`, `eastlong` and `westlong` classes.\n",
    "\n",
    "12. Then, we have created nine functions, that get all the survey information needed.\n",
    " \n",
    "13. Additionally, we created the nine function using the for loop to pass the 600 surveys URL and XML pages list into the mentioned above functions to return a list. We have passed the 600 surveys in three batches. Every batch has 200 surveys to avoid any block when using the request command.\n",
    "\n",
    "14. Finally, we create a function that writes all the info to a CSV file, and later we open it using pandas library as a data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download the webpage using `requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We installed and imported the requests library to download the web page.\n",
    "The library can be installed using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download a page, we can use the `get` function from requests, which returns a response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciences_data_url = 'https://www.usgs.gov/science-explorer-results?es=seismic+reflection&classification=data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = requests.get(sciences_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requests.get` returns a response object containing the data from the web pae and some other information.\n",
    "\n",
    "The `.status_code` property can be used to check if the request was successful. Asuccessful response will have [HTTP status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) an between 200 and 299."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or we can use the boolen\n",
    "responses.ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request was successful. We can get the contents of the page using `response.tet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = responses.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have checked the number of characters on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92192"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page contain over 92000 characters! Here are the firt 500 charaters of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML+RDFa 1.0//EN\"\\n  \"http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd\">\\n<html dir=\"ltr\" lang=\"en\" xml:lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\">\\n<head profile=\"http://www.w3.org/1999/xhtml/vocab\">\\n  <meta charset=\"utf-8\">\\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\\n  \\n  \\n  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\\n<script type=\"text/x-mathjax-c'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save it to a file and view the page locally within Jupyter using \"file>open\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('webpage.html', 'w') as f:\n",
    "    f.write(page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Used `Beautiful Soup` to parse the USGS web site and select the web site that contains the seismic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [USGS](https://www.usgs.gov/science-explorer-results?es=3D+Seismic+data&classification=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have scraped the Science Explorer web page to Exploring and get the web site that has the 2D and 3D Seismic Reflection Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We installed and imported the `beautifulsoup4` library to parse the web page.\n",
    "The library can be installed using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we parsered the web page and save the out put into a doc, which is a beautiful soup object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = BeautifulSoup(page_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exstracted the web pages titles using the `<h3>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_title_tags = doc.find_all('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3 class=\"list-title h4\">National Archive of Marine Seismic Surveys (NAMSS)</h3>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_title_tags[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_title_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3 class=\"list-title h4\">National Archive of Marine Seismic Surveys (NAMSS)</h3>,\n",
       " <h3 class=\"list-title h4\">Location of Seismic Reflection Line CRmv</h3>,\n",
       " <h3 class=\"list-title h4\">Bathymetry, acoustic backscatter, and minisparker seismic-reflection datasets collected southwest of Montague Island and southwest of Chenega, Alaska during field activity 2014-622-FA</h3>,\n",
       " <h3 class=\"list-title h4\">Marine geophysical data—Point Sal to Refugio State Beach, southern California</h3>,\n",
       " <h3 class=\"list-title h4\">Data report for line 8 of the 2011 USGS seismic imaging survey at San Andreas Lake, San Mateo County, California</h3>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_title_tags[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a for loop to collect all the titles available in the USGS web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['National Archive of Marine Seismic Surveys (NAMSS)',\n",
       " 'Location of Seismic Reflection Line CRmv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_title = []\n",
    "for tag in data_title_tags:\n",
    "    data_title.append(tag.text.strip())\n",
    "\n",
    "data_title[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data title fuction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create a function that gets the titles from all the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_title(tags):\n",
    "\n",
    "    #we have crated a for loop to get all the title tags  \n",
    "    data_title = []\n",
    "    for tag in data_title_tags:\n",
    "        data_title.append(tag.text.strip())\n",
    "\n",
    "    return data_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_title_tags_all = get_data_title(data_title_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['National Archive of Marine Seismic Surveys (NAMSS)',\n",
       " 'Location of Seismic Reflection Line CRmv']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_title_tags_all[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have use the `<div>` tag and class `<desc_selector>` to collect the data description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_selector = \"views-field views-field-drupal-contentfield-intro\"\n",
    "desc_tags = doc.find_all('div', {'class': desc_selector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(desc_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"views-field views-field-drupal-contentfield-intro\"> <span class=\"field-content\">The National Archive of Marine Seismic Surveys (NAMSS) is a marine seismic reflection data archive consisting of data acquired by or contributed to U.S. Department of the Interior agencies. The USGS is committed to preserving these data on behalf of the academic community and the nation. Data are provided with free and open access.\n",
       " </span> </div>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_tags[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"views-field views-field-drupal-contentfield-intro\"> <span class=\"field-content\">This dataset provides location information for the seismic reflection line CRmv across Crowleys Ridge in the New Madrid seismic zone, central US. The seismic reflection data are interpreted and discussed in the associated publication\n",
       " </span> </div>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_tags[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a for loop to get all the description from the web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This data release includes chirp seismic-reflection data collected in 2014 aboard the USGS\\xa0R/V Snavely\\xa0in San Pablo Bay, part of northern San Francisco Bay.\\xa0The\\xa0data were collected as part of USGS efforts to better understand the fault geometry of the Hayward and Rodgers Creek faults beneath the bay.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_desc= []\n",
    "for tag in desc_tags:\n",
    "    data_desc.append(tag.text.strip())\n",
    "    \n",
    "data_desc[5:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do a sanity check to make sure that all the length are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data description function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create a function that gets all the data description from all the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_desc(tags):\n",
    "    \n",
    "    data_desc =[]\n",
    "    # we have crated a for loop to get all the data description tags\n",
    "    for tag in desc_tags:\n",
    "        data_desc.append(tag.text.strip())\n",
    "        #data_desc = data_desc_1[0:20]\n",
    "    return data_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_desc_all =  get_data_desc(desc_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The National Archive of Marine Seismic Surveys (NAMSS) is a marine seismic reflection data archive consisting of data acquired by or contributed to U.S. Department of the Interior agencies. The USGS is committed to preserving these data on behalf of the academic community and the nation. Data are provided with free and open access.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_desc_all[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data page url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the `<div>` tag, class `<data_link_selector>` and `<herf>` to collect the needed urls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_link_selector = \"views-field green-link\"\n",
    "data_link_tags = doc.find_all('div', {'class': data_link_selector}, 'href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://walrus.wr.usgs.gov/namss/'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_link_tags[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_link_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"views-field green-link\"><a href=\"https://walrus.wr.usgs.gov/namss/\">https://walrus.wr.usgs.gov/namss/</a></div>,\n",
       " <div class=\"views-field green-link\"><a href=\"https://doi.org/10.5066/P9TFRP5D\">https://doi.org/10.5066/P9TFRP5D</a></div>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_link_tags[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_link_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://doi.org/10.5066/F74T6GF1'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_link_tags[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://walrus.wr.usgs.gov/namss/', 'https://doi.org/10.5066/P9TFRP5D']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_links = []\n",
    "\n",
    "for tag in data_link_tags:\n",
    "    data_links.append(tag.text.strip())\n",
    "    \n",
    "data_links[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data URLs function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create a function that gets all the data URLs from all the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_link_tags(tags):\n",
    "    \n",
    "    # we have crated a for loop to get all the data URLs tags\n",
    "    data_links = []\n",
    "    for tag in data_link_tags:\n",
    "        data_links.append(tag.text.strip())\n",
    "    \n",
    "    return data_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_links_all = get_data_link_tags(data_link_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://doi.org/10.5066/P9TFRP5D']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_links_all[1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page url function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have creted a function that wrap all the code in one step and get the url from the survey web sites which will be applied in future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_page(survey_url):\n",
    "    \n",
    "    # Download the url survey page\n",
    "    response = requests.get(survey_url) \n",
    "    \n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    \n",
    "    # Parse using Beautiful soup\n",
    "    page_url_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return page_url_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sciences_doc = get_url_page(sciences_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Science Explorer</title>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciences_doc.find('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the function `get_url_page` to download any web page and parse it using beautiful soup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 USGS web pages Data frame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we collected all the required information, we have used the `Pandas` library to create a data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the library inside the notbook use `pip` and then `import` to import it as pd for short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\r04ra18\\Anaconda3\\envs\\geocomp\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\r04ra18\\Anaconda3\\envs\\geocomp\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\Users\\r04ra18\\Anaconda3\\envs\\geocomp\\lib\\site-packages\\numpy\\.libs\\libopenblas.pyqhxlvvq7vesdpuvuadxevjobghjpay.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\\n%s\" %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a dictionary to store all the collected data and make it easy to change it to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'Data title' : data_title,\n",
    "    'Data Description' : data_desc,\n",
    "    'url' : data_links\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seismic survey data frame:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let us store our data in a data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data title</th>\n",
       "      <th>Data Description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>National Archive of Marine Seismic Surveys (NA...</td>\n",
       "      <td>The National Archive of Marine Seismic Surveys...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Location of Seismic Reflection Line CRmv</td>\n",
       "      <td>This dataset provides location information for...</td>\n",
       "      <td>https://doi.org/10.5066/P9TFRP5D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bathymetry, acoustic backscatter, and minispar...</td>\n",
       "      <td>High-resolution acoustic backscatter data, bat...</td>\n",
       "      <td>https://doi.org/10.5066/P9K1YQ35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marine geophysical data—Point Sal to Refugio S...</td>\n",
       "      <td>This data release includes approximately 1,032...</td>\n",
       "      <td>https://doi.org/10.5066/F7SX6BCD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data report for line 8 of the 2011 USGS seismi...</td>\n",
       "      <td>In June of 2011, the U.S. Geological Survey ac...</td>\n",
       "      <td>https://doi.org/10.5066/P9FX66OZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Chirp seismic-reflection data: San Pablo Bay, ...</td>\n",
       "      <td>This data release includes chirp seismic-refle...</td>\n",
       "      <td>https://doi.org/10.5066/F74T6GF1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>High-resolution seismic imaging of the West Na...</td>\n",
       "      <td>In November 2016, the U.S. Geological Survey a...</td>\n",
       "      <td>https://doi.org/10.5066/P9UREVME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>High-resolution seismic imaging of the West Na...</td>\n",
       "      <td>In November 2016, the U.S. Geological Survey a...</td>\n",
       "      <td>https://doi.org/10.5066/P92UWULX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Multichannel sparker seismic-reflection data o...</td>\n",
       "      <td>This data release contains high-resolution mul...</td>\n",
       "      <td>https://doi.org/10.5066/F7KP81BQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015 High Resolution Seismic Data Recorded at ...</td>\n",
       "      <td>In May 2015, we acquired high-resolution seism...</td>\n",
       "      <td>https://doi.org/10.5066/P9F4IAAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Continuous Seismic Profiling with a Dual-Frequ...</td>\n",
       "      <td>In May 2017 a dual-frequency echo sounder was ...</td>\n",
       "      <td>https://doi.org/10.5066/P9QS925T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Geoscience Data Viewer Web Mapping Application</td>\n",
       "      <td>Map viewer of the St. Petersburg Coastal and M...</td>\n",
       "      <td>https://usgs.maps.arcgis.com/apps/webappviewer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Continuous Seismic Profiling (CSP) at Callahan...</td>\n",
       "      <td>In May 2017 and July 2018, continuous seismic ...</td>\n",
       "      <td>https://doi.org/10.5066/P9QS925T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Release for the 2016 East Bay Seismic Ima...</td>\n",
       "      <td>In October 2016, we acquired an approximately ...</td>\n",
       "      <td>https://doi.org/10.5066/P941WN4P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Depth to Transition--Bolinas to Pescadero, Cal...</td>\n",
       "      <td>This part of DS 781 presents data for the dept...</td>\n",
       "      <td>https://doi.org/10.5066/F7891424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>California State Waters Map Series Data Catalog</td>\n",
       "      <td>This data catalog contains much of the data us...</td>\n",
       "      <td>https://doi.org/10.3133/ds781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Multibeam and multichannel sparker seismic-ref...</td>\n",
       "      <td>Multibeam bathymetry and multichannel sparker ...</td>\n",
       "      <td>https://doi.org/10.5066/F7NG4PTW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Transgressive Contours--Bolinas to Pescadero, ...</td>\n",
       "      <td>This part of DS 781 presents data for the dept...</td>\n",
       "      <td>https://doi.org/10.5066/F7891424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017 U.S. Geological Survey/BC Hydro seismic d...</td>\n",
       "      <td>In May 2017, we acquired high-resolution seism...</td>\n",
       "      <td>https://doi.org/10.5066/P9PRGZ53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Archive of digitized analog boomer seismic ref...</td>\n",
       "      <td>The U.S. Geological Survey (USGS) Coastal and ...</td>\n",
       "      <td>https://doi.org/10.5066/F7NP23H4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Data title  \\\n",
       "0   National Archive of Marine Seismic Surveys (NA...   \n",
       "1            Location of Seismic Reflection Line CRmv   \n",
       "2   Bathymetry, acoustic backscatter, and minispar...   \n",
       "3   Marine geophysical data—Point Sal to Refugio S...   \n",
       "4   Data report for line 8 of the 2011 USGS seismi...   \n",
       "5   Chirp seismic-reflection data: San Pablo Bay, ...   \n",
       "6   High-resolution seismic imaging of the West Na...   \n",
       "7   High-resolution seismic imaging of the West Na...   \n",
       "8   Multichannel sparker seismic-reflection data o...   \n",
       "9   2015 High Resolution Seismic Data Recorded at ...   \n",
       "10  Continuous Seismic Profiling with a Dual-Frequ...   \n",
       "11     Geoscience Data Viewer Web Mapping Application   \n",
       "12  Continuous Seismic Profiling (CSP) at Callahan...   \n",
       "13  Data Release for the 2016 East Bay Seismic Ima...   \n",
       "14  Depth to Transition--Bolinas to Pescadero, Cal...   \n",
       "15    California State Waters Map Series Data Catalog   \n",
       "16  Multibeam and multichannel sparker seismic-ref...   \n",
       "17  Transgressive Contours--Bolinas to Pescadero, ...   \n",
       "18  2017 U.S. Geological Survey/BC Hydro seismic d...   \n",
       "19  Archive of digitized analog boomer seismic ref...   \n",
       "\n",
       "                                     Data Description  \\\n",
       "0   The National Archive of Marine Seismic Surveys...   \n",
       "1   This dataset provides location information for...   \n",
       "2   High-resolution acoustic backscatter data, bat...   \n",
       "3   This data release includes approximately 1,032...   \n",
       "4   In June of 2011, the U.S. Geological Survey ac...   \n",
       "5   This data release includes chirp seismic-refle...   \n",
       "6   In November 2016, the U.S. Geological Survey a...   \n",
       "7   In November 2016, the U.S. Geological Survey a...   \n",
       "8   This data release contains high-resolution mul...   \n",
       "9   In May 2015, we acquired high-resolution seism...   \n",
       "10  In May 2017 a dual-frequency echo sounder was ...   \n",
       "11  Map viewer of the St. Petersburg Coastal and M...   \n",
       "12  In May 2017 and July 2018, continuous seismic ...   \n",
       "13  In October 2016, we acquired an approximately ...   \n",
       "14  This part of DS 781 presents data for the dept...   \n",
       "15  This data catalog contains much of the data us...   \n",
       "16  Multibeam bathymetry and multichannel sparker ...   \n",
       "17  This part of DS 781 presents data for the dept...   \n",
       "18  In May 2017, we acquired high-resolution seism...   \n",
       "19  The U.S. Geological Survey (USGS) Coastal and ...   \n",
       "\n",
       "                                                  url  \n",
       "0                   https://walrus.wr.usgs.gov/namss/  \n",
       "1                    https://doi.org/10.5066/P9TFRP5D  \n",
       "2                    https://doi.org/10.5066/P9K1YQ35  \n",
       "3                    https://doi.org/10.5066/F7SX6BCD  \n",
       "4                    https://doi.org/10.5066/P9FX66OZ  \n",
       "5                    https://doi.org/10.5066/F74T6GF1  \n",
       "6                    https://doi.org/10.5066/P9UREVME  \n",
       "7                    https://doi.org/10.5066/P92UWULX  \n",
       "8                    https://doi.org/10.5066/F7KP81BQ  \n",
       "9                    https://doi.org/10.5066/P9F4IAAL  \n",
       "10                   https://doi.org/10.5066/P9QS925T  \n",
       "11  https://usgs.maps.arcgis.com/apps/webappviewer...  \n",
       "12                   https://doi.org/10.5066/P9QS925T  \n",
       "13                   https://doi.org/10.5066/P941WN4P  \n",
       "14                   https://doi.org/10.5066/F7891424  \n",
       "15                      https://doi.org/10.3133/ds781  \n",
       "16                   https://doi.org/10.5066/F7NG4PTW  \n",
       "17                   https://doi.org/10.5066/F7891424  \n",
       "18                   https://doi.org/10.5066/P9PRGZ53  \n",
       "19                   https://doi.org/10.5066/F7NP23H4  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have scraped the first page, which contains 20 seismic data websites. However, this website contains several pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 USGS web pages CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have Created CSV file(s) with the extracted information to store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('data.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Getting seismic survey data out of seismic data page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 [NAMSS](https://walrus.wr.usgs.gov/namss/search/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have scraped the NAMSS web page which is the first site on the USGS web site to collect and get the requred information aboute 2D and 3D Seismic Reflection Data surveys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Location map showing USA costs margin and gulfs. The pink lines show the 2D seismic surveys, and the pink boxes show the 3D surveys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Location map.png](https://i.imgur.com/8sLmBfu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This web site is the first web site in our collected list in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_page_url = data_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://walrus.wr.usgs.gov/namss/'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_page_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the programmer inspect tool to identify the base URL and attached \"a\" to \"z\" characters to get all the surveys names. After that, we append them into one list, clean it and add the surveys names to the base URL to get all the surveys web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to add some extension to our URL to get the survey data page. After inspecting the developer tool, we need to add the filter, auto-complete and name id, and add the alphabet characters form (\"a\" to \"z\") to get the web pages where the data and information are hosted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Inspect.png](https://i.imgur.com/2xGF15F.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of testing the \"a\" character and check the URL will give the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic_data_page_url = data_links[0] + 'filter/autocomplete/?name=identifier&term=a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://walrus.wr.usgs.gov/namss/filter/autocomplete/?name=identifier&term=a'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seismic_data_page_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Used Beautiful Soup to parse the NAMSS web site and scrape the web site that contains the seismic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply the `requests` and `BeautifulSoup` to parse the web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(seismic_data_page_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the status code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13135"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic_data_doc = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Characters list form a to z :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After figuring out the base URL and extension, we can create a loop to iterate over the alphabetic characters to add them to the base URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of alphabite\n",
    "\n",
    "characs = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a loop to iterate over the characters list and it each character to the end of the base URL to create the surveys web sites URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://walrus.wr.usgs.gov/namss/filter/autocomplete/?name=identifier&term=a', 'https://walrus.wr.usgs.gov/namss/filter/autocomplete/?name=identifier&term=b', 'https://walrus.wr.usgs.gov/namss/filter/autocomplete/?name=identifier&term=c']\n"
     ]
    }
   ],
   "source": [
    "surveys_url = []\n",
    "\n",
    "for charac in characs:\n",
    "    surveys_url.append(data_links[0] + 'filter/autocomplete/?name=identifier&term=' + charac)\n",
    "\n",
    "print(surveys_url[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Survey url function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a function of for loop to get all the 26 surveys in the web site and get all thier responses to apply beutiful soup and scrape the web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_survey_names(url):\n",
    "\n",
    "    # loop over the survey url list and get all the responses\n",
    "    survey_names = []\n",
    "\n",
    "    for survey in surveys_url:\n",
    "        survey_names.append(requests.get(survey))\n",
    "\n",
    "    return survey_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_names_all = get_survey_names(surveys_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>]\n"
     ]
    }
   ],
   "source": [
    "print(survey_names_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic_survey_names = BeautifulSoup(response.text.strip(), 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append all the collected surveys in one large list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seismic_survey_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Clean survey URL list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since we get the survey names, we need to clean them and convert them to lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import and use the `golb` and `re` library to clean our survey URL list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_cleaned = [re.sub(r\" \", '', file) for file in seismic_survey_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like to track the cleaning steps, please uncomment the following cell to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surveys_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove the \\n, \" and [] ; then we need to split them to get list items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surveys_cleaned\n",
    "\n",
    "converted_list = []\n",
    "\n",
    "for element in surveys_cleaned:\n",
    "    converted_list.append(element.replace('\\n','').replace('\"', '').strip('[]').strip(). split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like to track the cleaning steps, please uncomment the following cell to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converted_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to remove the outer list to convert it from 2D list to 1D list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_outer(x):\n",
    "    while len(x) == 1 and isinstance(x[0], list):\n",
    "        x = x[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_list = strip_outer(converted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-645-FA_c'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we finished all the cleaning, we need to convert all the uppercase characters to lowercase characters then add them to the base URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_charac = []\n",
    "\n",
    "for survey in clean_list:\n",
    "    small_charac.append(survey.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(small_charac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(small_charac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://walrus.wr.usgs.gov/namss/'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_links[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a surveys data url, and start collecting our information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_data_link = []\n",
    "\n",
    "for survey in small_charac:\n",
    "    surveys_data_link.append(data_links[0] + 'survey/' + survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(surveys_data_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(surveys_data_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove corrupted web site:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the URL and XML have a corrupted web site so we have to remove them using the `del` and `remove` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://walrus.wr.usgs.gov/namss/survey/2014-645-fa_c',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/2014-645-fa_s',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/a-1-00-scmultichannel',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/a-1-00-scsinglechannel',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/a-1-02-sc_chirp',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/a-1-02-sc_gimcs',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/a-1-02-sc_htspark',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/a-1-02-sc_huntec',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/a-1-02-sc_msmcs',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/a-1-02-sc_scag']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surveys_data_link[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the `del` method to clean the list by indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "del surveys_data_link[2:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the `remove` method to clean the list by indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_data_link.remove('https://walrus.wr.usgs.gov/namss/survey/l-09-11-gamcs')\n",
    "surveys_data_link.remove('https://walrus.wr.usgs.gov/namss/survey/p1-13-lagreencanyon')\n",
    "surveys_data_link.remove('https://walrus.wr.usgs.gov/namss/survey/p1-13-lawalkerridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(surveys_data_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Survey navigation metadata:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used a for loop to create an XML list from the URL list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_xml = []\n",
    "for i in surveys_data_link:\n",
    "    survey_data_xml.append(i + '/metadata/seismic/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://walrus.wr.usgs.gov/namss/survey/2014-645-fa_c/metadata/seismic/',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/2014-645-fa_s/metadata/seismic/',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/a-1-02-sc_scms/metadata/seismic/',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/b-00-79-la/metadata/seismic/',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/b-00-95-la/metadata/seismic/',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/b-01-75-at/metadata/seismic/',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/b-01-77-la/metadata/seismic/',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/b-01-78-at/metadata/seismic/',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/b-01-80-at/metadata/seismic/',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/b-01-81-at/metadata/seismic/']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_data_xml[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(survey_data_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Collecting seismic survey data out of seismic data NAMSS web site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start and work on the first five data sites to collect the needed seismic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://walrus.wr.usgs.gov/namss/survey/2014-645-fa_c',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/2014-645-fa_s',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/a-1-02-sc_scms',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/b-00-79-la',\n",
       " 'https://walrus.wr.usgs.gov/namss/survey/b-00-95-la']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surveys_data_link[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic_survey_page_url_1 = surveys_data_link[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://walrus.wr.usgs.gov/namss/survey/2014-645-fa_c'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seismic_survey_page_url_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have applied a similar approach to the one we adopt in the previous steps to parse and request the web site information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_1 = requests.get(seismic_survey_page_url_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_1.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n  <meta charset=\"utf-8\">\\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n  <tit'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_1.text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10210"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses_1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_seismic_survey_data_page = responses_1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n  <meta charset=\"utf-8\">\\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n  <tit'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_seismic_survey_data_page[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('first_seismic_survey_page.html', 'w') as f:\n",
    "    f.write(first_seismic_survey_data_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Sismic data information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start collecting our seismic reflection data information now. We start by using `BeautifulSoup` to parser the web site and save it as a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_seismic_1 = BeautifulSoup(first_seismic_survey_data_page, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Survey names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows a ship tailing seismic acquisition recording strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![seismic acquisition.png](https://i.imgur.com/WbNxcKK.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We exstracted the web pages titles using the `<h1>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_names_tags = doc_seismic_1.find_all('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>2014-645-FA_c</h1>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_names_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-645-FA_c'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_names = survey_names_tags[0].text\n",
    "survey_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected the contributor of the seismic data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows the major seven sisters operators in the USA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Operator.png](https://i.imgur.com/mToqDb9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have use the `<div>` tag and class `<col-xs-12 col-sm-9>` to collect the operator name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"col-xs-12 col-sm-9\">\n",
       "<p>Chirp</p>\n",
       "</div>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operator_tags = doc_seismic_1.find_all('div', {'class':'col-xs-12 col-sm-9'})\n",
    "operator_tags[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pacific Coastal and Marine Science Center'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operator = operator_tags[2].text.strip()\n",
    "operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the `<div>` tag and class `<col-xs-12 col-sm-9>`  and `p` to collect the dates of the data acquisition. Then we index the fourth position, which always the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"col-xs-12 col-sm-9\">\n",
       "<p>Started on April 20, 2017, and ended on April 20, 2017.</p>\n",
       "</div>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_tags = doc_seismic_1.find_all('div', {'class':'col-xs-12 col-sm-9'}, 'p')\n",
    "dates_tags[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let just keep the year date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_type = dates_tags[3].text.strip()[-5:-1]\n",
    "dates_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 Data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seismic data either to be single-channel or multi-channel. The multi-channels either are 2D or 3D seismic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows seismic data acquisition on the surface and 2D seismic lines, and 3D seismic cube on the subsurface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Datatype.png](https://i.imgur.com/yEKtHQP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We index the first position to collect the data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"col-xs-12 col-sm-9\">\n",
       "<p>Singlechannel Seismic</p>\n",
       "</div>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type_tags = doc_seismic_1.find_all('div', {'class':'col-xs-12 col-sm-9'}, 'p')\n",
    "data_type_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Singlechannel Seismic'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type = data_type_tags[0].text.strip()\n",
    "data_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.5 Geographic Coordinate System (GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A coordinate system is any type of measuring system used to map space on either 2D or 3D surfaces. While the datum is the part of the GCS that determines which model (spheroid) is used to represent the earth's surface and where it is positioned relative to the surface. Here we refer to the GCS as datum in the following code since its most relaven to the seismic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GCS.png](https://i.imgur.com/9kQUbtL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let collect the datum information which is essintial for loading the seismic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"col-xs-12 col-sm-9\">\n",
       "<p>World Geodetic System 1984 (WGS84)</p>\n",
       "</div>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum_tags = doc_seismic_1.find_all('div', {'class':'col-xs-12 col-sm-9'}, 'p')\n",
    "datum_tags[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'World Geodetic System 1984 (WGS84)'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum = datum_tags[-1].text.strip()\n",
    "datum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GSC navigation information is essential for loading the seismic data in any other geological software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.6 Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Location.png](https://i.imgur.com/NfSXRHK.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we have used the navigation metadata web page to get the information about the location (latitude and longitude). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_page_xml = 'https://walrus.wr.usgs.gov/namss/survey/t-06-12-at/metadata/seismic/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_nav_xml = requests.get(nav_page_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_nav_xml.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_data_doc = BeautifulSoup(response_nav_xml.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the name `'northlat'`, `'southlat'`, `'eastlong'` and `'westlong'`  to collect the location information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "northlat_names_tags = nav_data_doc.find_all('northlat')\n",
    "nlat = northlat_names_tags[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "southlat_names_tags = nav_data_doc.find_all('southlat')\n",
    "slat = southlat_names_tags[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "eastlong_names_tags = nav_data_doc.find_all('eastlong')\n",
    "elong = eastlong_names_tags[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "westlong_names_tags = nav_data_doc.find_all('westlong')\n",
    "wlong = westlong_names_tags[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'36.92747, 36.12804, -74.33366, -74.90127'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_long = nlat + ', ' + slat + ', ' + elong + ', ' + wlong\n",
    "lat_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.7 Seismic data zip SEGY files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let grab the zip files of the data. We have two zip files that need to be collected: \n",
    "1. SEGY seismic data file. \n",
    "2. Navigation file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a download=\"\" href=\"https://walrus.wr.usgs.gov/namss/data/2014/namss.2014-645-FA.scs.chirp.zip\">Download</a>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seis_zip_files_tags = doc_seismic_1.find_all('a')\n",
    "seis_zip_files_tags[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://walrus.wr.usgs.gov/namss/data/2014/namss.2014-645-FA.scs.chirp.zip'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seis_zip_files = seis_zip_files_tags[10]['href']\n",
    "seis_zip_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.8 Size of the seismic file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now get the size on the seismic files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the `<span>` tag and index the first position to get the size of the file. Also, we have to clean the string using the `replace` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "seis_files_size_tags = doc_seismic_1.find_all('span')\n",
    "size_f = seis_files_size_tags[0].text\n",
    "size_seis = size_f.replace('\\xa0', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(20.5 MB)'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_seis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.9 Navigation data zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the `<a>` tag to collect the navigation data zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a download=\"\" href=\"/namss/media/navigation/2017/04/20/102230311158/2014-645-FA_chirp_nav.zip\">Download</a>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nav_zip_files_tags = doc_seismic_1.find_all('a')\n",
    "nav_zip_files_tags[12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to add our base url to get the full link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://walrus.wr.usgs.gov/namss/media/navigation/2017/04/20/102230311158/2014-645-FA_chirp_nav.zip'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url_zip = 'https://walrus.wr.usgs.gov'\n",
    "nav_zip_files = nav_zip_files_tags[12]\n",
    "full_navnav_zip = base_url_zip + nav_zip_files['href']\n",
    "full_navnav_zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.10 Size of the navigation zip file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the `<span>` tag to collect the navigation data zip files. Also, we have to clean the string using the `replace` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now get the size of the navigative files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_size_tags = doc_seismic_1.find_all('span')\n",
    "size_n = files_size_tags[1].text\n",
    "size_nav = size_n.replace('\\xa0', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(3.6 KB)'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_nav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Example on survey page number 20:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us choose survey number 20 and try to collect the information again to double-check and make sure everything goes well, and the workflow can be applied on another web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://walrus.wr.usgs.gov/namss/survey/b-01-95-la'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seismic_survey_page_url_20 = surveys_data_link[19]\n",
    "seismic_survey_page_url_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_20 = requests.get(seismic_survey_page_url_20)#\n",
    "responses_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweenty_seismic_survey_page = responses_20.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweenty_seismic_survey_page.html', 'w') as f:\n",
    "    f.write(tweenty_seismic_survey_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_seismic_20 = BeautifulSoup(tweenty_seismic_survey_page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey20_names_tags = doc_seismic_20.find_all('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>B-01-95-LA</h1>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey20_names_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-01-95-LA'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey20_names = survey20_names_tags[0].text\n",
    "survey20_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"col-xs-12 col-sm-9\">\n",
       "<p>Bureau of Ocean Energy Management</p>\n",
       "</div>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operator_tags20 = doc_seismic_20.find_all('div', {'class':'col-xs-12 col-sm-9'}, 'p')\n",
    "operator_tags20[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pacific Coastal and Marine Science Center'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operator = operator_tags[2].text.strip()\n",
    "operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"col-xs-12 col-sm-9\">\n",
       "<p>Started on Jan. 1, 1995, and ended on Jan. 1, 1995.</p>\n",
       "</div>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_tags20 = doc_seismic_20.find_all('div', {'class':'col-xs-12 col-sm-9'}, 'p')\n",
    "dates_tags20[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1995'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates20 = dates_tags20[3].text.strip()[-5:-1]\n",
    "dates20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"col-xs-12 col-sm-9\">\n",
       "<p>2D Multichannel Seismic</p>\n",
       "</div>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type_tags20 = doc_seismic_20.find_all('div', {'class':'col-xs-12 col-sm-9'}, 'p')\n",
    "data_type_tags20[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2D Multichannel Seismic'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type20 = data_type_tags20[0].text.strip()\n",
    "data_type20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"col-xs-12 col-sm-9\">\n",
       "<p>North American Datum 1927 (NAD27)</p>\n",
       "</div>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum_tags20 = doc_seismic_20.find_all('div', {'class':'col-xs-12 col-sm-9'}, 'p')\n",
    "datum_tags20[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'North American Datum 1927 (NAD27)'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum20 = datum_tags20[-1].text.strip()\n",
    "datum20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define helper functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Getting the survey page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will get the survey page converted to text which can be accessed locally for further operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surveys_url_doc(survey_url):\n",
    "    \n",
    "    # Download the url survey page\n",
    "    response = requests.get(survey_url) \n",
    "    \n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    \n",
    "    # Parse using Beautiful soup\n",
    "    survey_url_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return survey_url_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surveys_final_docs(survey_url):\n",
    "    survey_docs_lists = []\n",
    "    for i in range(len(survey_url)):\n",
    "        survey_docs_lists.append(get_surveys_url_doc(survey_url[i]))\n",
    "    return survey_docs_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Getting the navigation page:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will get the metadata page converted to text which can be accessed locally for further operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xml_servey_doc(survey_xml):\n",
    "    \n",
    "    # Download the xml survey page\n",
    "    response = requests.get(survey_xml) \n",
    "    \n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    \n",
    "    # parse using Beautiful soup\n",
    "    survey_xml_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return survey_xml_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Survey name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will get the survey name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_survey_name(survey_url_doc):\n",
    "    \n",
    "    # Get the survey name using the h1 tag\n",
    "    survey_names_tags = survey_url_doc.find_all('h1')\n",
    "    # indexing the 1st position to get the name\n",
    "    surveyname = survey_names_tags[0].text\n",
    "    \n",
    "    return surveyname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Survey information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will get the survey info such as operator, dates, datatype, and datum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_survey_info(survey_url_doc):\n",
    "    \n",
    "    # Get the operator name, dates, datum, using the div tag\n",
    "    info_tags = survey_url_doc.find_all('div', {'class':'col-xs-12 col-sm-9'})\n",
    "    # indexing the 3rd position to get the operator\n",
    "    operator = info_tags[2].text.strip()\n",
    "    # indexing the 4th position and cleaning to get the dates \n",
    "    dates = info_tags[3].text.strip()[-5:-1]\n",
    "    # indexing the 1st position to get the data type\n",
    "    datatype = info_tags[0].text.strip()\n",
    "    # indexing the last position to get the datum\n",
    "    datum = info_tags[-1].text.strip()\n",
    "    \n",
    "    return operator, dates, datatype, datum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Survey Latitude & Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will get the survey latitude and longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lnog(survey_xml_doc):\n",
    "    \n",
    "    # Get the lat and long from the xml page\n",
    "    northlat_names_tags = survey_xml_doc.find_all('northlat')\n",
    "    nlat = northlat_names_tags[0].text\n",
    "    southlat_names_tags = survey_xml_doc.find_all('southlat')\n",
    "    slat = southlat_names_tags[0].text\n",
    "    eastlong_names_tags = survey_xml_doc.find_all('eastlong')\n",
    "    elong = eastlong_names_tags[0].text\n",
    "    westlong_names_tags = survey_xml_doc.find_all('westlong')\n",
    "    wlong = westlong_names_tags[0].text\n",
    "    lat_long = nlat + ', ' + slat + ', ' + elong + ', ' + wlong\n",
    "    \n",
    "    return lat_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Size of the zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will get the size of the survey seismic and navigation zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_zip(survey_url_doc):\n",
    "    \n",
    "    # Get the size of the seismic segy and navigation data\n",
    "    seis_files_size_tags = survey_url_doc.find_all('span')\n",
    "    size_sf = seis_files_size_tags[0].text\n",
    "    size_nf = seis_files_size_tags[1].text\n",
    "    size_seisfile = size_sf.replace('\\xa0', ' ')\n",
    "    size_navfile = size_nf.replace('\\xa0', ' ')\n",
    "    \n",
    "    return size_seisfile, size_navfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will get the seismic and navigation zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip_files(survey_url_doc):\n",
    "        \n",
    "    # Get the seismic and navigation zip files\n",
    "    seis_zip_files_tags = survey_url_doc.find_all('a')\n",
    "    seiszip = seis_zip_files_tags[10]['href']\n",
    "    \n",
    "    # Get the matadata navigation zip file\n",
    "    base_url_zip = 'https://walrus.wr.usgs.gov'\n",
    "    nav_zip_files = seis_zip_files_tags[12]\n",
    "    navzip = base_url_zip + nav_zip_files['href']\n",
    "    \n",
    "    return seiszip, navzip    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper functions section (no.6) cover all the functions of the projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the complete code of the project in several functions, which will collect all the information from the 600 surveys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions will take the survey page URL and the metadata page XML and collect all needed information for the project. The final output of the functions will be a dictionary with the seismic surveys information and zip files which will be saved as a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Surveys URL pages as documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function will take the URL and get the `response` and save the collected page locally for further work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_survey_url_docs(survey_url):\n",
    "    \n",
    "        # Download the url survey page\n",
    "        response = requests.get(survey_url) \n",
    "\n",
    "        # Check successful response\n",
    "        if response.status_code != 200:\n",
    "            raise Exception('Failed to load page {}'.format(topic_url))\n",
    "\n",
    "        # Parse using Beautiful soup\n",
    "        survey_url_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        return survey_url_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will take a list of the 600 surveys and iterate over them using the `for` loop to create a document for each URL survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surveys_url_docs(survey_url_list):\n",
    "    surveys_url_docs = []\n",
    "    for i in range(len(survey_url_list)):\n",
    "        surveys_url_docs.append(create_survey_url_docs(survey_url_list[i]))\n",
    "    return surveys_url_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Surveys XML page as documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simmilar function will take the XML and get the `response` and save the collected page locally for further work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_survey_xml_docs(survey_xml):\n",
    "            \n",
    "        # Download the xml survey page\n",
    "        response_1 = requests.get(survey_xml) \n",
    "\n",
    "        # Check successful response\n",
    "        if response_1.status_code != 200:\n",
    "            raise Exception('Failed to load page {}'.format(topic_url))\n",
    "\n",
    "        # parse using Beautiful soup\n",
    "        survey_xml_doc = BeautifulSoup(response_1.text, 'html.parser')\n",
    "        \n",
    "        return survey_xml_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take a list of the 600 surveys and iterate over them using the `for` loop to create a document for each XML survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surveys_xml_docs(survey_xml_list):\n",
    "    surveys_xml_docs = []\n",
    "    for i in range(len(survey_xml_list)):\n",
    "        surveys_xml_docs.append(create_survey_xml_docs(survey_xml_list[i]))\n",
    "    return surveys_xml_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Surveys information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us use the URL document to collect the needed survey information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_survey_infos(survey_url_doc):  \n",
    "    \n",
    "        # Get the operator name, dates, datum, \n",
    "        survey_names_tags = survey_url_doc.find_all('h1')\n",
    "        surveyname = survey_names_tags[0].text\n",
    "        info_tags = survey_url_doc.find_all('div', {'class':'col-xs-12 col-sm-9'})\n",
    "        operator = info_tags[2].text.strip()\n",
    "        dates = info_tags[3].text.strip()[-5:-1]\n",
    "        datatype = info_tags[0].text.strip()\n",
    "        datum = info_tags[-1].text.strip()\n",
    "        \n",
    "        survey_infos_ls = [surveyname, operator, dates, datatype, datum]\n",
    "        \n",
    "        return survey_infos_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used a similar function that takes a list of the 600 surveys and iterate over them using the `for` loop to collect the information for each URL survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_survey_infos(survey_url_doc_list):\n",
    "    survey_infos = []\n",
    "    for i in range(len(survey_url_doc_list)):\n",
    "        survey_infos.append(create_survey_infos(survey_url_doc_list[i]))\n",
    "        \n",
    "    return survey_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Surveys datum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us collect the coordinates (latitude and longitude) for the surveys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lat_long(survey_xml_doc):\n",
    "    \n",
    "        # Get the lat and long from the xml page\n",
    "        northlat_names_tags = survey_xml_doc.find_all('northlat')\n",
    "        nlat = northlat_names_tags[0].text\n",
    "        southlat_names_tags = survey_xml_doc.find_all('southlat')\n",
    "        slat = southlat_names_tags[0].text\n",
    "        eastlong_names_tags = survey_xml_doc.find_all('eastlong')\n",
    "        elong = eastlong_names_tags[0].text\n",
    "        westlong_names_tags = survey_xml_doc.find_all('westlong')\n",
    "        wlong = westlong_names_tags[0].text\n",
    "        latlong_ls = [nlat, slat, elong, wlong]\n",
    "        #latlong_dict = {'North lat' : nlat, 'South lat' : slat, 'East long' : elong, 'West long' : wlong}\n",
    "        \n",
    "        return latlong_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used a similar function that takes a list of the 600 surveys and iterate over them using the `for` loop to collect the coordinates as latitude and longitude for each XML page survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_long(survey_xml_doc_list):\n",
    "    lat_long_ls = []\n",
    "    for i in range(len(survey_xml_doc_list)):\n",
    "        lat_long_ls.append(create_lat_long(survey_xml_doc_list[i]))\n",
    "    return lat_long_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Surveys files size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here lets collect the size of the seismic and navigation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_size_files(survey_url_doc):\n",
    "        # Get the size of the seismic segy and navigation data\n",
    "        seis_files_size_tags = survey_url_doc.find_all('span')\n",
    "        size_sf = seis_files_size_tags[0].text\n",
    "        size_nf = seis_files_size_tags[1].text\n",
    "        size_seisfile = size_sf.replace('\\xa0', ' ')\n",
    "        size_navfile = size_nf.replace('\\xa0', ' ')\n",
    "        #size_files_dict = {'SEGY size' : size_seisfile, 'Navigation size' : size_navfile}\n",
    "        size_files_ls = [size_seisfile, size_navfile]\n",
    "        return size_files_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used a function that takes a list of the 600 surveys and iterate over them using the `for` loop to collect the size of zip files for each survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size_files(survey_url_doc_list):\n",
    "    size_files_ls = []\n",
    "    for i in range(len(survey_url_doc_list)):\n",
    "        size_files_ls.append(create_size_files(survey_url_doc_list[i]))\n",
    "    return size_files_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Surveys zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to collect the zip file links which will directly download the seismic and navigation zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zip_files(survey_url_doc):\n",
    "    \n",
    "        # Get the seismic and navigation zip files\n",
    "        seis_zip_files_tags = survey_url_doc.find_all('a')\n",
    "        seiszip = seis_zip_files_tags[10]['href']\n",
    "\n",
    "        # Get the matadata navigation zip file\n",
    "        base_url_zip = 'https://walrus.wr.usgs.gov'\n",
    "        nav_zip_files = seis_zip_files_tags[12]\n",
    "        navzip = base_url_zip + nav_zip_files['href']\n",
    "        \n",
    "        #zip_files_dict = {'SEGY zip': seiszip, 'Navigation zip' : navzip, 'url' : survey_url}\n",
    "        zip_files_ls = [seiszip, navzip]\n",
    "        return zip_files_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used a similar function that takes a list of the 600 surveys and iterate over them using the `for` loop to collect the zip files for the seismic and navigation survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip_files(survey_url_doc_list):\n",
    "    zip_files_ls = []\n",
    "    for i in range(len(survey_url_doc_list)):\n",
    "        zip_files_ls.append(create_zip_files(survey_url_doc_list[i]))\n",
    "    return zip_files_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Surveys final data List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us add all the collected information in one list using the `append`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_data_list(survey_infos, survey_latlong, survey_sizefiles, survey_zip):\n",
    "\n",
    "    data_infos_ls = []\n",
    "\n",
    "    for i in range(len(survey_infos)):\n",
    "        data_infos_test = survey_infos[i] + survey_latlong[i] + survey_sizefiles[i] + survey_zip[i]\n",
    "        data_infos_ls.append(data_infos_test)\n",
    "    data_infos_ls\n",
    "    return data_infos_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 Surveys final data Dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create a dictionary to store all the collected data which make it easy to change it to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survey_data_dict(data_infos_ls):\n",
    "    survey_info_flist = []\n",
    "\n",
    "    for i in range(len(data_infos_ls)):\n",
    "\n",
    "            survey_info_dict ={\n",
    "                'Survey name' : data_infos_ls[i][0],\n",
    "                'Operator' : data_infos_ls[i][1],\n",
    "                'Dates' : data_infos_ls[i][2],\n",
    "                'Data type' : data_infos_ls[i][3], \n",
    "                'Datum' : data_infos_ls[i][4],\n",
    "                'North lat' : data_infos_ls[i][5],\n",
    "                'South lat' : data_infos_ls[i][6],\n",
    "                'East long' : data_infos_ls[i][7],\n",
    "                'West long' : data_infos_ls[i][8],\n",
    "                'SEGY size' : data_infos_ls[i][9],\n",
    "                'Navigation size' : data_infos_ls[i][10],\n",
    "                'SEGY zip': data_infos_ls[i][11],\n",
    "                'Navigation zip' : data_infos_ls[i][12],\n",
    "                            }\n",
    "            survey_info_flist.append(survey_info_dict)\n",
    "            \n",
    "    return survey_info_flist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.9 Surveys Data CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a CSV function to take the out dictionary from the function `surveys_final_data` to create file(s) with the extracted information to store the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data in csv\n",
    "\n",
    "import csv\n",
    "\n",
    "def write_csv(items, path):\n",
    "    \n",
    "    with open(path,'w', encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        # Write the headers in the first line\n",
    "        headers = list(items[0].keys())\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # Write one item per line\n",
    "        for item in items:\n",
    "            values = []\n",
    "            for header in headers:\n",
    "                values.append(str(item.get(header, \"\")))\n",
    "            writer.writerow(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.10 Section summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sections (No. 7) utilise nine functions to get the surveys information for all the 600 surveys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section utilises a set of functions from loops, functions and appends that take the surveys URL list and the XML list to iterate over these lists getting each URL/XML and pass it into the functions, resulting in a dictionary with the required information for all the 600 surveys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we presented the project results, which is seismic and navigation information and zip files of the 600 surveys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created three lists each list have 200 surveys URL and XML batches to pass it into the function `create_survey_info_dict` to avoid any block when using the request command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Generate the Seismic Surveys data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a list for the URL and XML surveys from 0 to 200 by indexing the main list `surveys_data_link`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_url_0_200 = surveys_data_link[0:200]\n",
    "survey_xml_0_200 = survey_data_xml[0:200]\n",
    "len(survey_url_0_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later we have pssed the new created lists into the nine funtions to collect all the required information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generated data for surveys 0 - 200:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the URL document. Notice that we have recorded the time needed to get the required information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "survey_data_url_doc_200 = get_surveys_url_docs(survey_url_0_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the XML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "survey_data_xml_doc_200 = get_surveys_xml_docs(survey_xml_0_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the survey information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_infos_200 = get_survey_infos(survey_data_url_doc_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_latlong_200 = get_lat_long(survey_data_xml_doc_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the files sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_sizefiles_200 = get_size_files(survey_data_url_doc_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_zipfiles_200 = get_zip_files(survey_data_url_doc_200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_flist_200 = final_data_list(\n",
    "    survey_data_infos_200,\n",
    "    survey_data_latlong_200,\n",
    "    survey_data_sizefiles_200,\n",
    "    survey_data_zipfiles_200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_final_dict_200 = survey_data_dict(survey_data_flist_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Survey name': 'B-00-79-LA',\n",
       " 'Operator': 'Bureau of Ocean Energy Management',\n",
       " 'Dates': '1979',\n",
       " 'Data type': '2D Multichannel Seismic',\n",
       " 'Datum': 'North American Datum 1927 (NAD27)',\n",
       " 'North lat': '28.00779',\n",
       " 'South lat': '27.94530',\n",
       " 'East long': '-92.09660',\n",
       " 'West long': '-92.18261',\n",
       " 'SEGY size': '(7.5 MB)',\n",
       " 'Navigation size': '(13.1 KB)',\n",
       " 'SEGY zip': 'https://walrus.wr.usgs.gov/namss/data/1979/namss.B-00-79-LA.mcs.vaporchoc.zip',\n",
       " 'Navigation zip': 'https://walrus.wr.usgs.gov/namss/media/navigation/2015/08/31/b-00-79-la.segp1'}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_final_dict_200[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generated data for surveys 200 - 400:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a list for the URL and XML surveys from 200 to 400 by indexing the main list `surveys_data_link`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_url_200_400 = surveys_data_link[200:400]\n",
    "survey_xml_200_400 = survey_data_xml[200:400]\n",
    "len(survey_url_200_400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "survey_data_url_doc_200_400 = get_surveys_url_docs(survey_url_200_400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "survey_data_xml_doc_200_400 = get_surveys_xml_docs(survey_xml_200_400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_infos_200_400 = get_survey_infos(survey_data_url_doc_200_400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_latlong_200_400 = get_lat_long(survey_data_xml_doc_200_400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_sizefiles_200_400 = get_size_files(survey_data_url_doc_200_400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_zipfiles_200_400 = get_zip_files(survey_data_url_doc_200_400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_flist_200_400 = final_data_list(\n",
    "    survey_data_infos_200_400,\n",
    "    survey_data_latlong_200_400,\n",
    "    survey_data_sizefiles_200_400,\n",
    "    survey_data_zipfiles_200_400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_final_dict_200_400 = survey_data_dict(survey_data_flist_200_400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Survey name': 'B-22-75-LA',\n",
       " 'Operator': 'Bureau of Ocean Energy Management',\n",
       " 'Dates': '1975',\n",
       " 'Data type': '2D Multichannel Seismic',\n",
       " 'Datum': 'North American Datum 1927 (NAD27)',\n",
       " 'North lat': '28.77294',\n",
       " 'South lat': '28.65648',\n",
       " 'East long': '-91.01254',\n",
       " 'West long': '-91.15930',\n",
       " 'SEGY size': '(41.4 MB)',\n",
       " 'Navigation size': '(8.1 KB)',\n",
       " 'SEGY zip': 'https://walrus.wr.usgs.gov/namss/data/1975/namss.B-22-75-LA.mcs.aquapulse.zip',\n",
       " 'Navigation zip': 'https://walrus.wr.usgs.gov/namss/media/navigation/2015/09/01/b-22-75-la.segp1'}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_final_dict_200_400[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generated data for surveys 400 - 600:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a list for the URL and XML surveys from 400 to 602 by indexing the main list `surveys_data_link`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_url_400_600 = surveys_data_link[400:602]\n",
    "survey_xml_400_600 = survey_data_xml[400:602]\n",
    "len(survey_url_400_600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "survey_data_url_doc_400_600 = get_surveys_url_docs(survey_url_400_600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "survey_data_xml_doc_400_600 = get_surveys_xml_docs(survey_xml_400_600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_infos_400_600 = get_survey_infos(survey_data_url_doc_400_600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_latlong_400_600 = get_lat_long(survey_data_xml_doc_400_600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_sizefiles_400_600 = get_size_files(survey_data_url_doc_400_600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_zipfiles_400_600 = get_zip_files(survey_data_url_doc_400_600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_flist_400_600 = final_data_list(\n",
    "    survey_data_infos_400_600,\n",
    "    survey_data_latlong_400_600,\n",
    "    survey_data_sizefiles_400_600,\n",
    "    survey_data_zipfiles_400_600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_final_dict_400_600 = survey_data_dict(survey_data_flist_400_600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Survey name': 'B-58-91-LA',\n",
       " 'Operator': 'Bureau of Ocean Energy Management',\n",
       " 'Dates': '1991',\n",
       " 'Data type': '3D Multichannel Seismic',\n",
       " 'Datum': 'North American Datum 1927 (NAD27)',\n",
       " 'North lat': '27.86193',\n",
       " 'South lat': '27.56530',\n",
       " 'East long': '-92.80660',\n",
       " 'West long': '-93.24867',\n",
       " 'SEGY size': '(27.0 GB)',\n",
       " 'Navigation size': '(2.6 KB)',\n",
       " 'SEGY zip': 'https://walrus.wr.usgs.gov/namss/data/1991/namss.B-58-91-LA.mcs3d.airgun.zip',\n",
       " 'Navigation zip': 'https://walrus.wr.usgs.gov/namss/media/navigation/2016/11/15/103155531173/B-58-91-LA.zip'}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_final_dict_400_600[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Survey data CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have Created CSV file(s) with the extracted surveys data information to store the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_0_200 = write_csv(survey_final_dict_200, 'surveydata_0_200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_200_400 = write_csv(survey_final_dict_200_400, 'surveydata_200_400.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data_400_600 = write_csv(survey_final_dict_400_600, 'surveydata_400_600.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Seismic Surveys Data frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created and saved the CSV files, we can access and present them using the `Pandas` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survey name</th>\n",
       "      <th>Operator</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Datum</th>\n",
       "      <th>North lat</th>\n",
       "      <th>South lat</th>\n",
       "      <th>East long</th>\n",
       "      <th>West long</th>\n",
       "      <th>SEGY size</th>\n",
       "      <th>Navigation size</th>\n",
       "      <th>SEGY zip</th>\n",
       "      <th>Navigation zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-00-79-LA</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1979</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1927 (NAD27)</td>\n",
       "      <td>28.00779</td>\n",
       "      <td>27.94530</td>\n",
       "      <td>-92.09660</td>\n",
       "      <td>-92.18261</td>\n",
       "      <td>(7.5 MB)</td>\n",
       "      <td>(13.1 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1979/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-00-95-LA</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1995</td>\n",
       "      <td>3D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1927 (NAD27)</td>\n",
       "      <td>28.32871</td>\n",
       "      <td>28.19741</td>\n",
       "      <td>-90.11074</td>\n",
       "      <td>-90.29505</td>\n",
       "      <td>(248.5 MB)</td>\n",
       "      <td>(1.4 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1995/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-01-75-AT</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1975</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1983 (NAD83)</td>\n",
       "      <td>40.58820</td>\n",
       "      <td>37.85201</td>\n",
       "      <td>-70.39941</td>\n",
       "      <td>-74.59488</td>\n",
       "      <td>(2.4 GB)</td>\n",
       "      <td>(564.6 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1975/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Survey name                           Operator  Dates  \\\n",
       "3  B-00-79-LA  Bureau of Ocean Energy Management   1979   \n",
       "4  B-00-95-LA  Bureau of Ocean Energy Management   1995   \n",
       "5  B-01-75-AT  Bureau of Ocean Energy Management   1975   \n",
       "\n",
       "                 Data type                              Datum  North lat  \\\n",
       "3  2D Multichannel Seismic  North American Datum 1927 (NAD27)   28.00779   \n",
       "4  3D Multichannel Seismic  North American Datum 1927 (NAD27)   28.32871   \n",
       "5  2D Multichannel Seismic  North American Datum 1983 (NAD83)   40.58820   \n",
       "\n",
       "   South lat  East long  West long   SEGY size Navigation size  \\\n",
       "3   27.94530  -92.09660  -92.18261    (7.5 MB)       (13.1 KB)   \n",
       "4   28.19741  -90.11074  -90.29505  (248.5 MB)        (1.4 KB)   \n",
       "5   37.85201  -70.39941  -74.59488    (2.4 GB)      (564.6 KB)   \n",
       "\n",
       "                                            SEGY zip  \\\n",
       "3  https://walrus.wr.usgs.gov/namss/data/1979/nam...   \n",
       "4  https://walrus.wr.usgs.gov/namss/data/1995/nam...   \n",
       "5  https://walrus.wr.usgs.gov/namss/data/1975/nam...   \n",
       "\n",
       "                                      Navigation zip  \n",
       "3  https://walrus.wr.usgs.gov/namss/media/navigat...  \n",
       "4  https://walrus.wr.usgs.gov/namss/media/navigat...  \n",
       "5  https://walrus.wr.usgs.gov/namss/media/navigat...  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's jest view three rows\n",
    "df_200 = pd.read_csv('surveydata_0_200.csv')\n",
    "df_200[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survey name</th>\n",
       "      <th>Operator</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Datum</th>\n",
       "      <th>North lat</th>\n",
       "      <th>South lat</th>\n",
       "      <th>East long</th>\n",
       "      <th>West long</th>\n",
       "      <th>SEGY size</th>\n",
       "      <th>Navigation size</th>\n",
       "      <th>SEGY zip</th>\n",
       "      <th>Navigation zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-21-83-LA</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1983</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1927 (NAD27)</td>\n",
       "      <td>29.17737</td>\n",
       "      <td>26.09115</td>\n",
       "      <td>-87.55914</td>\n",
       "      <td>-93.41922</td>\n",
       "      <td>(3.6 GB)</td>\n",
       "      <td>(2.9 MB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1983/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-21-88-LA</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1988</td>\n",
       "      <td>3D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1927 (NAD27)</td>\n",
       "      <td>28.21156</td>\n",
       "      <td>28.07931</td>\n",
       "      <td>-90.07547</td>\n",
       "      <td>-90.22775</td>\n",
       "      <td>(1.2 GB)</td>\n",
       "      <td>(1.4 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1988/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-22-75-AT</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1975</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>World Geodetic System 1984 (WGS84)</td>\n",
       "      <td>39.47816</td>\n",
       "      <td>37.27581</td>\n",
       "      <td>-72.48754</td>\n",
       "      <td>-74.97537</td>\n",
       "      <td>(1.0 GB)</td>\n",
       "      <td>(182.0 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1975/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Survey name                           Operator  Dates  \\\n",
       "0  B-21-83-LA  Bureau of Ocean Energy Management   1983   \n",
       "1  B-21-88-LA  Bureau of Ocean Energy Management   1988   \n",
       "2  B-22-75-AT  Bureau of Ocean Energy Management   1975   \n",
       "\n",
       "                 Data type                               Datum  North lat  \\\n",
       "0  2D Multichannel Seismic   North American Datum 1927 (NAD27)   29.17737   \n",
       "1  3D Multichannel Seismic   North American Datum 1927 (NAD27)   28.21156   \n",
       "2  2D Multichannel Seismic  World Geodetic System 1984 (WGS84)   39.47816   \n",
       "\n",
       "   South lat  East long  West long SEGY size Navigation size  \\\n",
       "0   26.09115  -87.55914  -93.41922  (3.6 GB)        (2.9 MB)   \n",
       "1   28.07931  -90.07547  -90.22775  (1.2 GB)        (1.4 KB)   \n",
       "2   37.27581  -72.48754  -74.97537  (1.0 GB)      (182.0 KB)   \n",
       "\n",
       "                                            SEGY zip  \\\n",
       "0  https://walrus.wr.usgs.gov/namss/data/1983/nam...   \n",
       "1  https://walrus.wr.usgs.gov/namss/data/1988/nam...   \n",
       "2  https://walrus.wr.usgs.gov/namss/data/1975/nam...   \n",
       "\n",
       "                                      Navigation zip  \n",
       "0  https://walrus.wr.usgs.gov/namss/media/navigat...  \n",
       "1  https://walrus.wr.usgs.gov/namss/media/navigat...  \n",
       "2  https://walrus.wr.usgs.gov/namss/media/navigat...  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_200_400 = pd.read_csv('surveydata_200_400.csv')\n",
    "df_200_400[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survey name</th>\n",
       "      <th>Operator</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Datum</th>\n",
       "      <th>North lat</th>\n",
       "      <th>South lat</th>\n",
       "      <th>East long</th>\n",
       "      <th>West long</th>\n",
       "      <th>SEGY size</th>\n",
       "      <th>Navigation size</th>\n",
       "      <th>SEGY zip</th>\n",
       "      <th>Navigation zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-58-79-LA</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1979</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1927 (NAD27)</td>\n",
       "      <td>29.16374</td>\n",
       "      <td>27.84176</td>\n",
       "      <td>-89.76667</td>\n",
       "      <td>-93.46160</td>\n",
       "      <td>(61.9 MB)</td>\n",
       "      <td>(168.1 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1979/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-58-83-LA</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1983</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1927 (NAD27)</td>\n",
       "      <td>29.11513</td>\n",
       "      <td>28.11947</td>\n",
       "      <td>-91.81355</td>\n",
       "      <td>-93.28320</td>\n",
       "      <td>(44.0 MB)</td>\n",
       "      <td>(318.9 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1983/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-58-84-LA</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1984</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1927 (NAD27)</td>\n",
       "      <td>28.50671</td>\n",
       "      <td>28.42055</td>\n",
       "      <td>-92.11479</td>\n",
       "      <td>-92.26291</td>\n",
       "      <td>(11.7 MB)</td>\n",
       "      <td>(7.3 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1984/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Survey name                           Operator  Dates  \\\n",
       "0  B-58-79-LA  Bureau of Ocean Energy Management   1979   \n",
       "1  B-58-83-LA  Bureau of Ocean Energy Management   1983   \n",
       "2  B-58-84-LA  Bureau of Ocean Energy Management   1984   \n",
       "\n",
       "                 Data type                              Datum  North lat  \\\n",
       "0  2D Multichannel Seismic  North American Datum 1927 (NAD27)   29.16374   \n",
       "1  2D Multichannel Seismic  North American Datum 1927 (NAD27)   29.11513   \n",
       "2  2D Multichannel Seismic  North American Datum 1927 (NAD27)   28.50671   \n",
       "\n",
       "   South lat  East long  West long  SEGY size Navigation size  \\\n",
       "0   27.84176  -89.76667  -93.46160  (61.9 MB)      (168.1 KB)   \n",
       "1   28.11947  -91.81355  -93.28320  (44.0 MB)      (318.9 KB)   \n",
       "2   28.42055  -92.11479  -92.26291  (11.7 MB)        (7.3 KB)   \n",
       "\n",
       "                                            SEGY zip  \\\n",
       "0  https://walrus.wr.usgs.gov/namss/data/1979/nam...   \n",
       "1  https://walrus.wr.usgs.gov/namss/data/1983/nam...   \n",
       "2  https://walrus.wr.usgs.gov/namss/data/1984/nam...   \n",
       "\n",
       "                                      Navigation zip  \n",
       "0  https://walrus.wr.usgs.gov/namss/media/navigat...  \n",
       "1  https://walrus.wr.usgs.gov/namss/media/navigat...  \n",
       "2  https://walrus.wr.usgs.gov/namss/media/navigat...  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_400_600 = pd.read_csv('surveydata_400_600.csv')\n",
    "df_400_600[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Merge Dataframes in one CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we have created a final CSV file for all the 600 surveys which can be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survey name</th>\n",
       "      <th>Operator</th>\n",
       "      <th>Dates</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Datum</th>\n",
       "      <th>North lat</th>\n",
       "      <th>South lat</th>\n",
       "      <th>East long</th>\n",
       "      <th>West long</th>\n",
       "      <th>SEGY size</th>\n",
       "      <th>Navigation size</th>\n",
       "      <th>SEGY zip</th>\n",
       "      <th>Navigation zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-01-75-AT</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1975</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1983 (NAD83)</td>\n",
       "      <td>40.58820</td>\n",
       "      <td>37.85201</td>\n",
       "      <td>-70.39941</td>\n",
       "      <td>-74.59488</td>\n",
       "      <td>(2.4 GB)</td>\n",
       "      <td>(564.6 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1975/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-01-77-LA</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1977</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1927 (NAD27)</td>\n",
       "      <td>28.95265</td>\n",
       "      <td>28.57102</td>\n",
       "      <td>-89.17095</td>\n",
       "      <td>-89.37113</td>\n",
       "      <td>(15.4 MB)</td>\n",
       "      <td>(11.3 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1977/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-01-78-AT</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1978</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1983 (NAD83)</td>\n",
       "      <td>39.87120</td>\n",
       "      <td>39.59591</td>\n",
       "      <td>-72.03049</td>\n",
       "      <td>-72.47954</td>\n",
       "      <td>(16.9 MB)</td>\n",
       "      <td>(9.2 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1978/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B-01-80-AT</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1980</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1983 (NAD83)</td>\n",
       "      <td>36.35034</td>\n",
       "      <td>30.44691</td>\n",
       "      <td>-74.22113</td>\n",
       "      <td>-80.29978</td>\n",
       "      <td>(2.4 GB)</td>\n",
       "      <td>(542.7 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1980/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B-01-81-AT</td>\n",
       "      <td>Bureau of Ocean Energy Management</td>\n",
       "      <td>1981</td>\n",
       "      <td>2D Multichannel Seismic</td>\n",
       "      <td>North American Datum 1983 (NAD83)</td>\n",
       "      <td>41.01726</td>\n",
       "      <td>30.18925</td>\n",
       "      <td>-66.81061</td>\n",
       "      <td>-79.63947</td>\n",
       "      <td>(4.4 GB)</td>\n",
       "      <td>(970.9 KB)</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/data/1981/nam...</td>\n",
       "      <td>https://walrus.wr.usgs.gov/namss/media/navigat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Survey name                           Operator  Dates  \\\n",
       "5  B-01-75-AT  Bureau of Ocean Energy Management   1975   \n",
       "6  B-01-77-LA  Bureau of Ocean Energy Management   1977   \n",
       "7  B-01-78-AT  Bureau of Ocean Energy Management   1978   \n",
       "8  B-01-80-AT  Bureau of Ocean Energy Management   1980   \n",
       "9  B-01-81-AT  Bureau of Ocean Energy Management   1981   \n",
       "\n",
       "                 Data type                              Datum  North lat  \\\n",
       "5  2D Multichannel Seismic  North American Datum 1983 (NAD83)   40.58820   \n",
       "6  2D Multichannel Seismic  North American Datum 1927 (NAD27)   28.95265   \n",
       "7  2D Multichannel Seismic  North American Datum 1983 (NAD83)   39.87120   \n",
       "8  2D Multichannel Seismic  North American Datum 1983 (NAD83)   36.35034   \n",
       "9  2D Multichannel Seismic  North American Datum 1983 (NAD83)   41.01726   \n",
       "\n",
       "   South lat  East long  West long  SEGY size Navigation size  \\\n",
       "5   37.85201  -70.39941  -74.59488   (2.4 GB)      (564.6 KB)   \n",
       "6   28.57102  -89.17095  -89.37113  (15.4 MB)       (11.3 KB)   \n",
       "7   39.59591  -72.03049  -72.47954  (16.9 MB)        (9.2 KB)   \n",
       "8   30.44691  -74.22113  -80.29978   (2.4 GB)      (542.7 KB)   \n",
       "9   30.18925  -66.81061  -79.63947   (4.4 GB)      (970.9 KB)   \n",
       "\n",
       "                                            SEGY zip  \\\n",
       "5  https://walrus.wr.usgs.gov/namss/data/1975/nam...   \n",
       "6  https://walrus.wr.usgs.gov/namss/data/1977/nam...   \n",
       "7  https://walrus.wr.usgs.gov/namss/data/1978/nam...   \n",
       "8  https://walrus.wr.usgs.gov/namss/data/1980/nam...   \n",
       "9  https://walrus.wr.usgs.gov/namss/data/1981/nam...   \n",
       "\n",
       "                                      Navigation zip  \n",
       "5  https://walrus.wr.usgs.gov/namss/media/navigat...  \n",
       "6  https://walrus.wr.usgs.gov/namss/media/navigat...  \n",
       "7  https://walrus.wr.usgs.gov/namss/media/navigat...  \n",
       "8  https://walrus.wr.usgs.gov/namss/media/navigat...  \n",
       "9  https://walrus.wr.usgs.gov/namss/media/navigat...  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_600 = pd.concat([df_200, df_200_400, df_400_600])\n",
    "df_all_600[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the final survey data file for all the 600 surveys which is the main result of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_600.to_csv('surveydata_all_600.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sharing results as CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output results of the project as CSV files will be available alongside the notebook for all students, researchers and companies staff geoscientists to use and interpret 2D and 3D seismic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you interpret seismic data and want to share your results with the broader geoscientist community, please use the [Virtual Seismic Atlas](https://www.seismicatlas.org/) (VSA) web site by [Professor Robert Butler](https://www.abdn.ac.uk/people/rob.butler/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we have presented a novel Python coding-driven approach that scrapes survey data on the USGS web site to collect information about subsurface seismic reflection 2D and 3D datasets. The seismic surveys data on the USGS web site were captured, scraped and presented using code from well-known scraping libraries: `Requests` and `BeautifulSoup`. \n",
    "\n",
    "\n",
    "Application of the web scraping workflow on the NAMSS seismic survey dataset showed its efficacy in collecting, simplifying, and visualising the most critical data and highlighted discrepancies between serial surveys with subsurface 2D and 3D datasets. \n",
    "\n",
    "\n",
    "The workflow has been automated using python code functions, which provide collections of the surveys information and categorises zip files of seismic data for future research and interpretations.\n",
    "\n",
    "We discuss the main benefits of the project; these include:\n",
    "\n",
    "\n",
    "\n",
    "1. Web scraping for data collections can be made prior to any data analysis or interpretation, allowing the interpreter to know the limitation and uncertainty in the data or increase the data they are interpreting in. \n",
    "\n",
    "\n",
    "2. Tabulated data in data frame format allow a numerical analysis of data information which allow assessments to be made of interpretation efficacy and can be used to highlight anomalies between serial surveys datasets and with seismic or navigation data files.  \n",
    "\n",
    "\n",
    "3. Web scraping fields can be user-defined and used to develop data acquisition strategies to improve data sharing and decrease overall data inaccessibility. \n",
    "\n",
    "\n",
    "\n",
    "4. The automated web scrape approach provides user-controlled, quick and easy data collections of the sub-surface data and associated geological models. It could be developed further to provide refined data collection and for broader automated analysis such as machine learning. \n",
    "\n",
    "\n",
    "\n",
    "Our automated web scraping project on Jovian is open source and freely available. \n",
    "\n",
    "All the code, functions, results and project workflow can be effectively applied to other websites and subsurface geological datasets to provide insight into data availability from a data-accessibility viewpoint. \n",
    "\n",
    "The potential to integrate the project workflow can be presented as a back-end of other methods of data analysis, e.g. machine learning, multiple interpretation analysis and stochastic methods that could produce a further contribution to the solution of data limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ramysaleem](https://i.imgur.com/D6HESuc.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has successfully collected and explored 600 seismic reflection surveys from the USGS web site on the coast of the USA. However, many geosciences open-source data need to be explored, collected and made more accessible to the researcher, such as collecting the geosciences data such as well data and seismic data on the Geological Survey of the Netherlands, UK, Germany and Norway. \n",
    "\n",
    "\n",
    "Here are a few web sites that could be a **potential science web scraping projects** that can make our science open-source data more accessible.\n",
    "\n",
    "\n",
    "1. Well data on the USGS web site. (https://www.data.bsee.gov/).\n",
    "\n",
    "\n",
    "\n",
    "2. Geosciences data on the Geological Survey of the Netherlands (https://www.nlog.nl/datacenter/).\n",
    "\n",
    "\n",
    "\n",
    "3. Geosciences data on the British Geological Survey (https://www.bgs.ac.uk/geological-data/opengeoscience/).\n",
    "\n",
    "\n",
    "4. Geosciences data on the Geological Survey of Germany (https://www.bgr.bund.de/EN/Home/homepage_node_en.html;jsessionid=6C08E0F50D81D1FDFCA4D4BBFC5A315E.2_cid331).\n",
    "\n",
    "\n",
    "\n",
    "5. Geosciences data on the Geological Survey of Norway (https://www.ngu.no/en/topic/datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Acknowledgement & References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to express my special thanks of gratitude to my supervisors [Dr Clare Bond](https://www.abdn.ac.uk/people/clare.bond/) and [Professor Rob Butler](https://www.abdn.ac.uk/people/rob.butler), alongside with [Mr. Aakash N S](https://aakashns.medium.com/?source=collection_about-------------------------------------) and [Jovian team](https://blog.jovian.ai/about) who gave me the golden opportunity to do this wonderful project on the topic of web scraping, which also helped me in doing a lot of research that will be part of my final PhD thesis.\n",
    "\n",
    "This project was carried out as part of my machine learning data collection technique and part of a University of Aberdeen provided PhD supported by The NERC Centre for Doctoral Training in Oil & Gas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Web scraping notebook by [Aakash N S](https://aakashns.medium.com/?source=collection_about-------------------------------------).\n",
    "\n",
    "\n",
    "\n",
    "2. Workshop on `Web-Scraping` by Aakash N S, [Let's Build a Python Web Scraping Project from Scratch | Hands-On Tutorial](https://www.youtube.com/watch?v=RKsLLG-bzEY&t=6677s).\n",
    "\n",
    "\n",
    "\n",
    "3. [`BeautifulSoup`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) documentation library.\n",
    "\n",
    "\n",
    "\n",
    "4. [`Scrapy`](https://docs.scrapy.org/en/latest/) documentation library.\n",
    "\n",
    "\n",
    "\n",
    "5. [USGS science explorer](https://www.usgs.gov/science/science-explorer/Geology). \n",
    "\n",
    "\n",
    "\n",
    "6. [USGS seismic reflection data](https://www.usgs.gov/science-explorer-results?es=3D+Seismic+data&classification=data).\n",
    "\n",
    "\n",
    "\n",
    "7. [The National Archive of Marine Seismic Surveys NAMSS](https://walrus.wr.usgs.gov/namss/).\n",
    "\n",
    "\n",
    "\n",
    "8. [3D seismic profile animation](https://www.usgs.gov/media/images/3d-seismic-profile-animation).\n",
    "\n",
    "\n",
    "\n",
    "9. [How to Purchase Property using Web Scraping - Using Python, Beautiful Soup and Pandas by Pritesh Patel](https://blog.jovian.ai/how-to-purchase-property-using-web-scraping-1d7448ef6c2d). \n",
    "\n",
    "\n",
    "\n",
    "10. [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).\n",
    "\n",
    "\n",
    "\n",
    "11. [Geophysical imaging](https://en.wikipedia.org/wiki/Geophysical_imaging).\n",
    "\n",
    "\n",
    "\n",
    "12. [Coordinate Systems: What's the Difference?](https://www.esri.com/arcgis-blog/products/arcgis-pro/mapping/coordinate-systems-difference/#:~:text=A%20datum%20is%20one%20parameter,positioned%20relative%20to%20the%20surface.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![footer.png](https://i.imgur.com/T1d4vMI.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geocomp",
   "language": "python",
   "name": "geocomp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
